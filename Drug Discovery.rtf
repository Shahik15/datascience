{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;\f2\fmodern\fcharset0 Courier-Oblique;
}
{\colortbl;\red255\green255\blue255;\red9\green9\blue9;\red0\green29\blue164;\red17\green109\blue18;
\red82\green0\blue135;\red121\green121\blue121;\red18\green51\blue230;\red0\green0\blue109;}
{\*\expandedcolortbl;;\csgenericrgb\c3529\c3529\c3529;\csgenericrgb\c0\c11373\c64314;\csgenericrgb\c6667\c42745\c7059;
\csgenericrgb\c32157\c0\c52941;\csgenericrgb\c47451\c47451\c47451;\csgenericrgb\c7059\c20000\c90196;\csgenericrgb\c0\c0\c42745;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs26 \cf2 \

\f1 \cf3 import \cf2 os\
os.environ[\cf4 "OPENAI_API_KEY"\cf2 ] = \cf4 \'93key here\'93\
\
\cf3 from \cf2 llama_index.core \cf3 import \cf2 VectorStoreIndex, SimpleDirectoryReader,Settings\
\cf3 from \cf2 llama_index.core.embeddings \cf3 import \cf2 resolve_embed_model\
\cf3 from \cf2 llama_index.llms.ollama \cf3 import \cf2 Ollama\
\cf3 from \cf2 llama_index.embeddings.huggingface \cf3 import \cf2 HuggingFaceEmbedding\
\cf3 from \cf2 llama_index.llms.huggingface \cf3 import \cf2 HuggingFaceLLM\
\cf3 import \cf2 chromadb\
\cf3 from \cf2 llama_index.vector_stores.chroma \cf3 import \cf2 ChromaVectorStore\
\cf3 from \cf2 llama_index.core \cf3 import \cf2 StorageContext\
\
chroma_client= chromadb.PersistentClient()\
chroma_collection= chroma_client.create_collection(\cf4 "quickstart"\cf2 )\
vector_store= ChromaVectorStore(\cf5 chroma_collection\cf2 =chroma_collection)\
storage_context= StorageContext.from_defaults(\cf5 vector_store\cf2 =vector_store)\
\
\
\
documents= SimpleDirectoryReader(\cf4 '/Users/shahikkhan/PycharmProjects/pythonProject/Testing/'\cf2 ).load_data()\

\f2\i \cf6 #bge embedding model\
#Settings.embed_model= resolve_embed_model("local:BAAI/bge-small-en-v1.5")\
\

\f1\i0 \cf2 Settings.chunk_size=\cf7 512\
\cf3 from \cf2 llama_index.core.node_parser \cf3 import \cf2 SentenceSplitter\
index= VectorStoreIndex.from_documents(documents,\cf5 transformations\cf2 =[SentenceSplitter(\cf5 chunk_size\cf2 =\cf7 512\cf2 )],\cf5 storage_context\cf2 =storage_context\
                                       ,\cf5 show_progress\cf2 =\cf3 True\cf2 )\
\
Settings.embed_model = HuggingFaceEmbedding(\cf5 model_name\cf2 =\cf4 "BAAI/bge-small-en-v1.5"\cf2 )\

\f2\i \cf6 #Settings.llm = HuggingFaceLLM(model_name="HuggingFaceH4/zephyr-7b-alpha",generate_kwargs=\{"temperature": 0.7, "do_sample": False\},)\
#ollama\

\f1\i0 \cf2 Settings.llm= Ollama(\cf5 model\cf2 =\cf4 "llama3.1"\cf2 , \cf5 request_timeout\cf2 =\cf7 30.0\cf2 )\

\f2\i \cf6 #index= VectorStoreIndex.from_documents(documents,)\
\

\f1\i0 \cf2 query_engine= index.as_query_engine(\cf5 similarity_top_k\cf2 =\cf7 5\cf2 ,\cf5 response_mode\cf2 =\cf4 "tree_summarize"\cf2 ,\cf5 streaming\cf2 =\cf3 True\cf2 )\
response= query_engine.query(\cf4 "Solve water allocation problem?"\cf2 )\
\cf8 print\cf2 (response)\
\
\
}